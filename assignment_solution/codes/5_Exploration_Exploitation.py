# -*- coding: utf-8 -*-
# @File    : 5_Exploration_Exploitation.py
# @Author  : Hua Guo
# @Disc    :
import logging
import numpy as np
import pandas as pd
from typing import Tuple, List
from abc import ABCMeta
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm


class Env:
    def __init__(self, k_arms: int, total_steps: int) -> None:
        self.k_arms = k_arms
        self.total_steps = total_steps
        self.q_mean_lst = 2*np.random.random(self.k_arms)-1 # range: [-1, 1]
        self.best_action = np.argmax(self.q_mean_lst)
        self.optimal_mean_reward = self.q_mean_lst[self.best_action]
        self.norm_scale = 1
        self.finished_steps = 0
        logging.info(f"Q mean list: {self.q_mean_lst}")

    def step(self, action: int) -> Tuple[float, bool]:
        assert action < self.k_arms
        reward = np.random.normal(loc=self.q_mean_lst[action], scale=self.norm_scale)
        self.finished_steps += 1
        done = False
        if self.finished_steps >= self.total_steps:
            done = True
        return reward, done

    def reset(self):
        self.finished_steps = 0


class Agent(metaclass=ABCMeta):
    def choose_action(self, **kwargs):
        pass

    def learn(self, **kwargs):
        pass


class EpsilonGreedyAgent(Agent):
    def __init__(self, epsilon: float, k_arms: int) -> None:
        self.epsilon = epsilon
        self.current_rewards = [0] * k_arms
        self.visited_times = [0]*k_arms
        self.k_arms = k_arms

    def choose_action(self) -> int:
        if np.random.rand() <= self.epsilon:
            return np.random.choice(range(self.k_arms))
        else:
            return np.argmax(self.current_rewards)

    def learn(self, action: int, reward: float) -> None:
        self.current_rewards[action] += 1/(self.visited_times[action]+1)*(reward-self.current_rewards[action])
        self.visited_times[action] += 1


class UCBAgent(Agent):
    def __init__(self, c_params: float, k_arms: int) -> float:
        self.c_params = c_params
        self.q_a = np.array([0] * k_arms)
        self.visited_times = np.array([0]*k_arms)
        self.k_arms = k_arms
        self.total_actions = 0

    def choose_action(self) -> int:
        q_lst = self.q_a + self.c_params*np.sqrt(np.log(self.total_actions+1)/(self.visited_times+1))
        print(q_lst,np.argmax(q_lst))
        return np.argmax(q_lst)

        # self.current_rewards = self.q_a
        # if np.random.rand() <= 0.1:
        #     return np.random.choice(range(self.k_arms))
        # else:
        #     return np.argmax(self.current_rewards)

    def learn(self, action: int, reward: float) -> None:
        # self.q_a[action]
        q_a_lst = self.q_a.tolist()
        q_a_lst[action] = self.q_a.tolist()[action] + 1/(self.visited_times[action]+1)*(reward-self.q_a[action])

        self.q_a = np.array(q_a_lst)
        print(self.q_a)
        self.visited_times[action] += 1
        self.total_actions += 1


def line_plot(dic: dict, y_label: str, xlabel='Simulation Num'):
    sns.lineplot(data=pd.DataFrame(dic))
    plt.xlabel(xlabel=xlabel)
    plt.ylabel(ylabel=y_label)
    plt.title(y_label)
    plt.savefig(y_label+'.png')
    plt.show()


def simulation(env: Env, agent: Agent, episode: int) -> List:
    env.reset()
    # optimal_action_frac = np.array([0])
    # reward_single_game = np.array([])
    all_oaf = []
    all_trl = []
    optimal_action_frac = [0]
    total_regret_lst = [0]
    for _ in tqdm(range(episode)):
        while True:
            action = agent.choose_action()
            if action == env.best_action:
                match = 1
            else:
                match = 0
            length = len(optimal_action_frac)
            optimal_action_frac.append(((optimal_action_frac[-1]*length)+match)/(length+1))
            reward, done = env.step(action=action)
            total_regret_lst.append(total_regret_lst[-1] + env.optimal_mean_reward - reward)
            agent.learn(action=action, reward=reward)
            if done:
                break
            # logging.debug(f"Episode: {idx+1}; reward: {reward}")
        all_oaf.append(optimal_action_frac)
        all_trl.append(total_regret_lst)
    oaf_array = np.array(all_oaf)
    trl_array = np.array(all_trl)
    oaf = np.mean(oaf_array, axis=0).tolist()
    trl = np.mean(trl_array, axis=0).tolist()
    return trl, oaf


if __name__ == "__main__":
    # Config
    debug = True
    k_arms = 10
    episode = 10
    if debug:
        total_steps = 100
        logging.basicConfig(level='DEBUG')
    else:
        logging.basicConfig(level='INFO')
        total_steps = 2500
    # main code
    env = Env(k_arms=k_arms, total_steps=total_steps)
    agent_dict = {
        # 'E-agent_0.1': EpsilonGreedyAgent(epsilon=0.1, k_arms=k_arms),
        'UCB_2': UCBAgent(c_params=2, k_arms=k_arms)
        # , 'E-agent_0.2': EpsilonGreedyAgent(epsilon=0.2, k_arms=k_arms)
        # , 'E-agent_0.5': EpsilonGreedyAgent(epsilon=0.5, k_arms=k_arms)
    }
    regrets_dict = {}
    percent_dict = {}
    for key, value in agent_dict.items():
        regrets, percent = simulation(env=env, agent=value, episode=episode)
        regrets_dict[key] = regrets
        percent_dict[key] = percent
    # print(regrets_dict)
    # print(percent_dict)
    # line_plot(dic=regrets_dict, y_label='expected_total_regret')
    line_plot(dic=percent_dict, y_label='percentage_correct_decisions')






